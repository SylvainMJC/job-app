name: Unified CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  # Permet de déclencher manuellement le workflow
  workflow_dispatch:

jobs:
  backend-build:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
    
    - name: Grant execute permission for gradlew
      run: chmod +x ./backend/gradlew
    
    - name: Build backend
      run: |
        cd backend
        ./gradlew bootJar
        # Verify build artifacts
        echo "Build artifacts:"
        ls -la build/libs/
    
    - name: Upload backend build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: backend-build
        path: backend/build/libs/
        retention-days: 1

  backend-unit-tests:
    needs: [backend-build]
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
    
    - name: Grant execute permission for gradlew
      run: chmod +x ./backend/gradlew
    
    # Télécharger les artefacts de build pour éviter de recompiler
    - name: Download backend build
      uses: actions/download-artifact@v4
      with:
        name: backend-build
        path: backend/build/libs/
    
    # Vérification des tests unitaires disponibles
    - name: List unit test files
      run: find backend/src/test/unit -name "*.java" | sort
    
    - name: Run unit tests
      run: cd backend && ./gradlew unitTest -i
    
    - name: Generate JaCoCo report for unit tests
      run: cd backend && ./gradlew jacocoUnitTestReport
    
    - name: Upload JaCoCo unit test reports
      uses: actions/upload-artifact@v4
      with:
        name: jacoco-unit-reports
        path: backend/build/reports/jacoco/unit/
        retention-days: 7

  backend-integration-tests:
    needs: [backend-build]
    runs-on: ubuntu-latest
    
    # Configurer Docker-in-Docker pour les tests d'intégration
    services:
      dind:
        image: docker:dind
        options: --privileged
        ports:
          - 2375:2375
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
    
    - name: Grant execute permission for gradlew
      run: chmod +x ./backend/gradlew
    
    # Télécharger les artefacts de build pour éviter de recompiler
    - name: Download backend build
      uses: actions/download-artifact@v4
      with:
        name: backend-build
        path: backend/build/libs/
    
    # Configuration de Docker pour Testcontainers
    - name: Set up Docker
      run: |
        docker info
        echo "TESTCONTAINERS_HOST_OVERRIDE=localhost" >> $GITHUB_ENV
    
    # Vérification des tests d'intégration disponibles
    - name: List integration test files
      run: find backend/src/test/integration -name "*.java" | sort
    
    - name: Run integration tests
      run: cd backend && ./gradlew integrationTest -i
      continue-on-error: true  # Allow tests to fail but continue the workflow
    
    - name: Generate JaCoCo report for integration tests
      run: cd backend && ./gradlew jacocoIntegrationTestReport
      continue-on-error: true
    
    - name: Upload JaCoCo integration test reports
      uses: actions/upload-artifact@v4
      with:
        name: jacoco-integration-reports
        path: backend/build/reports/jacoco/integration/
        retention-days: 7
      if: always()  # Always upload even if tests fail
  
  frontend-build:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    # Setup Node.js pour le frontend
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install frontend dependencies
      run: cd frontend && npm install
    
    - name: Build frontend
      run: cd frontend && npm run build
    
    - name: Upload frontend build
      uses: actions/upload-artifact@v4
      with:
        name: frontend-build
        path: frontend/dist/
        retention-days: 1

  frontend-unit-tests:
    needs: [frontend-build]
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    # Setup Node.js pour le frontend
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install frontend dependencies
      run: cd frontend && npm install
    
    # Run frontend unit tests
    - name: Test frontend
      run: cd frontend && npm run test:unit
      continue-on-error: true
    
    - name: Upload unit test report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: frontend-coverage
        path: frontend/coverage/
        retention-days: 7

  # Job pour les tests de performance avec JMeter
  performance-tests:
    runs-on: ubuntu-latest
    needs: [backend-build]
    
    # Configurer Docker-in-Docker pour exécuter JMeter et PostgreSQL
    services:
      dind:
        image: docker:dind
        options: --privileged
        ports:
          - 2375:2375
      # PostgreSQL pour les tests
      postgres:
        image: postgres:14
        env:
          POSTGRES_DB: jobapp
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker
      uses: docker/setup-buildx-action@v2
      with:
        driver-opts: network=host
    
    - name: Download backend build
      uses: actions/download-artifact@v4
      with:
        name: backend-build
        path: backend/build/libs/
    
    - name: Set execute permission for jar
      run: chmod +x backend/build/libs/*.jar
    
    - name: Install PostgreSQL client
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client
    
    - name: Start backend for performance tests
      run: |
        cd backend
        
        # Collect environment info for diagnostics
        echo "Environment diagnostics:"
        java -version
        echo "Free disk space:"
        df -h
        echo "Memory:"
        free -h || echo "free command not available"
        echo "Network interfaces:"
        ip addr || ifconfig || echo "Network command not available"
        
        # Démarrer avec paramètres simplifiés et base de données PostgreSQL pour les tests
        echo "Starting backend for performance tests on port 8080..."
        
        # Vérifier la disponibilité de PostgreSQL
        echo "Checking PostgreSQL connectivity..."
        pg_isready -h localhost -p 5432 || echo "PostgreSQL not ready yet"
        
        # List available JAR files for diagnostic purposes
        echo "Available JAR files:"
        ls -la build/libs/
        
        # Use only the executable JAR (not the -plain.jar)
        EXECUTABLE_JAR=$(find build/libs/ -name "*.jar" -not -name "*-plain.jar" -type f | head -n 1)
        echo "Using executable JAR: $EXECUTABLE_JAR"
        
        if [ -z "$EXECUTABLE_JAR" ]; then
          echo "ERROR: No executable JAR found!"
          exit 1
        fi
        
        nohup java -jar "$EXECUTABLE_JAR" \
          --server.port=8080 \
          --spring.datasource.url=jdbc:postgresql://localhost:5432/jobapp \
          --spring.datasource.username=postgres \
          --spring.datasource.password=postgres \
          --spring.datasource.driver-class-name=org.postgresql.Driver \
          --spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect \
          --spring.jpa.hibernate.ddl-auto=update \
          --logging.level.org.springframework=INFO \
          --logging.level.org.hibernate=INFO \
          > perf-test-app.log 2>&1 &
        echo $! > app.pid
        
        # Attendre que l'application démarre
        echo "Waiting for application to start..."
        max_attempts=60  # Increased wait time: 60 attempts * 3 seconds = 180 seconds
        attempt=1
        backend_started=false
        
        while [ $attempt -le $max_attempts ]; do
          echo "Checking if app is running (attempt $attempt of $max_attempts)..."
          if curl -s http://localhost:8080/api/offres > /dev/null; then
            echo "Application is up and running!"
            backend_started=true
            break
          fi
          
          # Check logs for specific patterns every few attempts
          if [ $((attempt % 5)) -eq 0 ]; then
            echo "--- Log snapshot at attempt $attempt ---"
            tail -n 50 perf-test-app.log
            
            # Check for common startup errors in the log
            if grep -q "APPLICATION FAILED TO START" perf-test-app.log; then
              echo "ERROR: Application failed to start. See logs below:"
              cat perf-test-app.log
              exit 1
            fi
          fi
          
          attempt=$((attempt+1))
          sleep 3
        done
        
        # Display application logs for debugging
        echo "Application logs:"
        cat perf-test-app.log
        
        # Fail the build if the backend didn't start
        if [ "$backend_started" != "true" ]; then
          echo "ERROR: Backend application did not start within the allocated time"
          ps aux | grep java
          echo "Port 8080 status:"
          netstat -tulpn | grep 8080 || ss -tulpn | grep 8080 || echo "Port status command not available"
          exit 1
        fi
        
        # Verify application startup
        echo "Verifying application fully started..."
        if grep -q "Started JobAppApplication" perf-test-app.log || grep -q "Started Application" perf-test-app.log; then
          echo "Application startup successful!"
        else
          echo "WARNING: Application startup message not found, but endpoint is accessible."
        fi
    
    - name: Run simplified performance test
      run: |
        # Verify connectivity before running tests
        echo "Verifying backend connectivity before tests..."
        retry_count=0
        max_retries=3
        
        while [ $retry_count -lt $max_retries ]; do
          if curl -s http://localhost:8080/api/offres > /dev/null; then
            echo "✅ Backend is accessible!"
            break
          else
            retry_count=$((retry_count+1))
            if [ $retry_count -ge $max_retries ]; then
              echo "❌ ERROR: Backend is not accessible after $max_retries attempts. Cannot run performance tests."
              echo "Last curl attempt:"
              curl -v http://localhost:8080/api/offres
              exit 1
            fi
            echo "⚠️ Backend not accessible, retrying in 5 seconds (attempt $retry_count/$max_retries)..."
            sleep 5
          fi
        done
        
        # Créer le répertoire pour les rapports
        mkdir -p backend/src/test/jmeter/reports
        
        # Exécuter un test curl simple pour vérifier la connectivité
        echo "Testing direct API call:"
        curl -v http://localhost:8080/api/offres
        
        # Construire et exécuter un conteneur simplifié
        cd backend/src/test/jmeter
        echo "Building JMeter Docker container..."
        docker build -t jmeter-tests .
        
        # Exécuter le test de performance avec --network=host
        echo "Running JMeter with host network..."
        set -o pipefail  # Make sure pipeline failures are caught
        docker run --network host \
          -v ${{ github.workspace }}/backend/src/test/jmeter/reports:/jmeter/reports \
          jmeter-tests \
          "localhost" "8080" "http" "2" "1" "3" | tee jmeter-output.log
        
        jmeter_result=$?
        echo "JMeter test exit code: $jmeter_result"
        
        # Check for error messages in the output
        if [ $jmeter_result -ne 0 ] || grep -q "Connection refused" jmeter-output.log; then
          echo "ERROR: Performance test failed or connection was refused"
          cat jmeter-output.log
          exit 1
        fi
        
        echo "Performance test completed successfully"
    
    - name: Capture logs after tests
      if: always()
      run: |
        echo "Application logs after tests:"
        cat backend/perf-test-app.log
        
        # Arrêter l'application backend
        if [ -f backend/app.pid ]; then
          echo "Stopping backend application..."
          kill $(cat backend/app.pid) || echo "Failed to stop application"
        fi
    
    - name: Archive performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-reports
        path: backend/src/test/jmeter/reports/
        retention-days: 7
    
    - name: Generate performance summary
      if: always()
      run: |
        echo "# Performance Test Summary" > performance-summary.md
        echo "## Test Configuration" >> performance-summary.md
        echo "- Threads: 10" >> performance-summary.md
        echo "- Ramp-up: 5 seconds" >> performance-summary.md
        echo "- Duration: 30 seconds" >> performance-summary.md
        echo "" >> performance-summary.md
        echo "## Results" >> performance-summary.md
        
        if [ -f backend/src/test/jmeter/reports/statistics.json ]; then
          echo "- Status: Success" >> performance-summary.md
          echo "- [Detailed Report Available](../artifacts/performance-test-reports/index.html)" >> performance-summary.md
        else
          echo "- Status: Failed" >> performance-summary.md
          echo "- No statistics available" >> performance-summary.md
        fi
    
    - name: Upload performance summary
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-summary
        path: performance-summary.md
        retention-days: 7

  # Job pour les tests end-to-end avec Playwright
  e2e-tests:
    runs-on: ubuntu-latest
    needs: [backend-build, frontend-build]
    
    # Configurer Docker-in-Docker pour exécuter Playwright
    services:
      dind:
        image: docker:dind
        options: --privileged
        ports:
          - 2375:2375
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install dependencies
      run: cd frontend && npm install
    
    - name: Install Playwright browsers
      run: cd frontend && npx playwright install --with-deps chromium
    
    - name: Download backend build
      uses: actions/download-artifact@v4
      with:
        name: backend-build
        path: backend/build/libs/
    
    - name: Set execute permission for jar
      run: chmod +x backend/build/libs/*.jar
    
    - name: Start backend for E2E tests
      run: |
        cd backend
        java -jar build/libs/*.jar &
        echo "Waiting for backend to start..."
        sleep 30
      env:
        SPRING_PROFILES_ACTIVE: test
    
    - name: Download frontend build
      uses: actions/download-artifact@v4
      with:
        name: frontend-build
        path: frontend/dist/
    
    - name: Start the frontend
      run: |
        cd frontend
        npm run preview &
        echo "Waiting for frontend to start..."
        sleep 10
    
    - name: Run E2E tests
      run: cd frontend && npm run test:e2e
      env:
        E2E_BASE_URL: http://localhost:4173
        PLAYWRIGHT_DOCKER_HOST: "localhost:2375"
      continue-on-error: true
    
    - name: Upload Playwright report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: playwright-report
        path: frontend/playwright-report/
        retention-days: 7

  # Rapport de qualité code et déploiement (Jobs fusionnés)
  code-quality-and-deploy:
    needs: [backend-unit-tests, backend-integration-tests, frontend-unit-tests, e2e-tests, performance-tests]
    runs-on: ubuntu-latest
    if: always() # Run even if some tests failed
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download JaCoCo unit test reports
      uses: actions/download-artifact@v4
      with:
        name: jacoco-unit-reports
        path: reports/jacoco/unit
      continue-on-error: true
    
    - name: Download JaCoCo integration test reports
      uses: actions/download-artifact@v4
      with:
        name: jacoco-integration-reports
        path: reports/jacoco/integration
      continue-on-error: true
    
    - name: Download frontend unit test reports
      uses: actions/download-artifact@v4
      with:
        name: frontend-coverage
        path: reports/frontend-coverage
      continue-on-error: true
    
    - name: Download performance test summary
      uses: actions/download-artifact@v4
      with:
        name: performance-summary
        path: reports/performance
      continue-on-error: true
    
    - name: Generate summary
      run: |
        echo "# Test Coverage Summary" > coverage-summary.md
        echo "## Backend Unit Tests" >> coverage-summary.md
        echo "- OffreTest: Test des propriétés et de l'égalité des offres" >> coverage-summary.md
        echo "- OffreControllerTest: Test des méthodes du contrôleur avec des mocks" >> coverage-summary.md
        echo "" >> coverage-summary.md
        echo "## Backend Integration Tests" >> coverage-summary.md
        echo "- OffreControllerIT: Test du contrôleur avec la base de données réelle" >> coverage-summary.md
        echo "- OffreRepositoryIT: Test du repository avec une base PostgreSQL" >> coverage-summary.md
        echo "" >> coverage-summary.md
        echo "## Frontend Unit Tests" >> coverage-summary.md
        echo "- LoginView.spec.js: Test du composant de login" >> coverage-summary.md
        echo "- OffresView.spec.js: Test du composant d'affichage des offres" >> coverage-summary.md
        echo "" >> coverage-summary.md
        echo "## Performance Tests" >> coverage-summary.md
        if [ -f reports/performance/performance-summary.md ]; then
          cat reports/performance/performance-summary.md >> coverage-summary.md
        else
          echo "- Performance test results not available" >> coverage-summary.md
        fi
    
    - name: Upload summary
      uses: actions/upload-artifact@v4
      with:
        name: test-summary
        path: coverage-summary.md
        retention-days: 7
    
    # Déploiement uniquement lorsque push sur main
    - name: Download backend build for deployment
      if: github.ref == 'refs/heads/main'
      uses: actions/download-artifact@v4
      with:
        name: backend-build
        path: backend-build
    
    - name: Download frontend build for deployment
      if: github.ref == 'refs/heads/main'
      uses: actions/download-artifact@v4
      with:
        name: frontend-build
        path: frontend-dist
    
    - name: Display test summary
      run: cat coverage-summary.md
    
    # Optionnel: Si vous avez un serveur de déploiement, ajoutez les étapes ici
    - name: Deploy to production
      if: github.ref == 'refs/heads/main'
      run: echo "Déploiement en production - Simulation"
      
    - name: Notify deployment success
      if: github.ref == 'refs/heads/main'
      run: |
        echo "Application déployée avec succès!"
        echo "Version: $(date +%Y%m%d-%H%M%S)" 